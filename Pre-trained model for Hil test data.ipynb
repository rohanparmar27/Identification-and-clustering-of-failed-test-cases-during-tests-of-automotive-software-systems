{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566d874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv1D, Conv1DTranspose, Dense, MaxPooling1D,Flatten, UpSampling1D,Embedding, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, SGD\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer\n",
    "from keras.callbacks import EarlyStopping,  ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3802b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "# Load the CAE model\n",
    "autoencoder = load_model(\"Final_cae_model_Result16.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84009752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model from disk\n",
    "loaded_kmeans = joblib.load('kmeans_model_final8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ab3f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Car-To-Car Rear braking: Approach to braking target with 50 km/h.', 'The distance defined via Alias leads to a distance of 40 m between front bumper of ASM vehicle and rear bumper of fellow vehicle.', 'Evaluation CustomVerifying, whether the normalized score of the AEB system exceeds the value 0.5. The randomly chosen value is not part of the', 'Verifying, whether the normalized score of the AEB system exceeds the value 0.5. The randomly chosen value is not part of the NCAP assessment protocol.', 'Verifying, whether the lateral deviation is within tolerance.Test: CCRTest', 'The distance defined via Alias leads to a distance of 12 m between front bumper of ASM vehicle and rear bumper of fellow vehicle.', 'Verifying, whether the lateral deviation is within tolerance.Concrete Test Case: 1']\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import json\n",
    "\n",
    "# Open the PDF file\n",
    "with open('report.pdf', 'rb') as pdf_file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    # Get the total number of pages\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Convert PDF to JSON\n",
    "    data = []\n",
    "    for page in pdf_reader.pages:\n",
    "        data.append(page.extract_text())\n",
    "   \n",
    "\n",
    "\n",
    "with open('pdf_to_json.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Load JSON data\n",
    "with open('pdf_to_json.json') as f:\n",
    "    pdf_data = json.load(f)\n",
    "\n",
    "# Extract comments\n",
    "comments = []\n",
    "keyword = \"Comment\"\n",
    "min_comment_length = 20\n",
    "for page in pdf_data:\n",
    "    if keyword in page:\n",
    "        comments_on_page = page.split(keyword)[1:]\n",
    "        for comment in comments_on_page:\n",
    "            comment = comment.strip()\n",
    "            if '\\n' in comment:\n",
    "                comment = comment.split('\\n')[0]\n",
    "            if ' C:\\\\' in comment:\n",
    "                continue\n",
    "            comment = re.sub(r'^\\d+\\s*', '', comment)\n",
    "            if comment not in comments and len(comment) >= min_comment_length:\n",
    "                comments.append(comment)\n",
    "\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32760301",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 48\n",
    "\"\"\"Function for preprocessing the text.\"\"\"\n",
    "stemmer = SnowballStemmer('english')\n",
    "def preprocess_text(text, stop_words):\n",
    "    \n",
    "    # Check if the text is not a string (e.g., NaN) and return an empty string in such cases\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "     # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0f6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cartocar rear braking approach braking target kmh', 'distance defined via alias leads distance front bumper asm vehicle rear bumper fellow vehicle', 'evaluation customverifying whether normalized score aeb system exceeds value randomly chosen value part', 'verifying whether normalized score aeb system exceeds value randomly chosen value part ncap assessment protocol', 'verifying whether lateral deviation within tolerancetest ccrtest', 'distance defined via alias leads distance front bumper asm vehicle rear bumper fellow vehicle', 'verifying whether lateral deviation within toleranceconcrete test case']\n",
      "1/1 [==============================] - 0s 154ms/step\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "comments_pre = [preprocess_text(comment, stop_words) for comment in comments]\n",
    "print(comments_pre)\n",
    "# Convert the external text data into sequences of integers\n",
    "X_external = tokenizer.texts_to_sequences(comments_pre)\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "encoded_external = pad_sequences(X_external, padding='post', maxlen=max_length)\n",
    "new_seq= np.expand_dims(encoded_external, axis=-1)\n",
    "reconstructed_text= autoencoder.predict(new_seq)\n",
    "exp_reconstructed_text = np.reshape(reconstructed_text, newshape=(reconstructed_text.shape[0], -1))\n",
    "y_ext_pred = loaded_kmeans.predict(exp_reconstructed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d675f5c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car-To-Car Rear braking: Approach to braking target with 50 km/h.: Sensor Failure\n",
      "The distance defined via Alias leads to a distance of 40 m between front bumper of ASM vehicle and rear bumper of fellow vehicle.: Sensor Failure\n",
      "Evaluation CustomVerifying, whether the normalized score of the AEB system exceeds the value 0.5. The randomly chosen value is not part of the: Sensor Failure\n",
      "Verifying, whether the normalized score of the AEB system exceeds the value 0.5. The randomly chosen value is not part of the NCAP assessment protocol.: Sensor Failure\n",
      "Verifying, whether the lateral deviation is within tolerance.Test: CCRTest: Sensor Failure\n",
      "The distance defined via Alias leads to a distance of 12 m between front bumper of ASM vehicle and rear bumper of fellow vehicle.: Sensor Failure\n",
      "Verifying, whether the lateral deviation is within tolerance.Concrete Test Case: 1: Sensor Failure\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to map cluster labels to text labels\n",
    "label_map = {0: 'Hardware Failure', 1: 'Software Failure', 2:'Sensor Failure', 3:'network Failure'}\n",
    "\n",
    "# Convert the cluster labels from numbers to text\n",
    "text_labels = [label_map[label] for label in y_ext_pred]\n",
    "\n",
    "# Print the cluster labels as text\n",
    "for i in range(len(comments)):\n",
    "    print('{}: {}'.format(comments[i], text_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8eb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f097d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f099f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
