{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import davies_bouldin_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bf45e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function for preprocessing the text.\"\"\"\n",
    "stemmer = SnowballStemmer('english')\n",
    "def preprocess_text(text, stop_words):\n",
    "    \n",
    "    # Check if the text is not a string (e.g., NaN) and return an empty string in such cases\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "     # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c84a6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "MainDf = pd.read_csv('merged csv.csv', encoding='cp1252', on_bad_lines='skip')\n",
    "MainDf.drop_duplicates(inplace=True)\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data = np.array(MainDf['description'].apply(lambda x: preprocess_text(x, stop_words)))\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4d7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 48\n",
    "# Load Labeled Data and Preprocess\n",
    "df = pd.read_csv('Test_data.csv')\n",
    "Labels = df['type']\n",
    "Label_data = df['description'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "lb_sequences = tokenizer.texts_to_sequences(Label_data)\n",
    "lb_padded = pad_sequences(lb_sequences, padding='post', maxlen=max_len)\n",
    "Nor_lb_padded = normalize(lb_padded, axis=1)\n",
    "lb_new_data = np.reshape(Nor_lb_padded, (len(Nor_lb_padded), Nor_lb_padded.shape[1], 1))\n",
    "label_encoder = LabelEncoder()\n",
    "labels_Int = label_encoder.fit_transform(Labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792f5562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "# Load the CAE model\n",
    "encoder = load_model(\"Final_cae_model_Result16.keras\")\n",
    "\n",
    "Lb_encoded_test = encoder.predict(lb_new_data)\n",
    "exp_lb_encoded_test = np.reshape(Lb_encoded_test, newshape=(Lb_encoded_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe36dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 48)\n"
     ]
    }
   ],
   "source": [
    "kmeans = joblib.load('kmeans_model_final8.pkl')\n",
    "\n",
    "lb_test_cluster_labels =  kmeans.predict(exp_lb_encoded_test)\n",
    "\n",
    "print(Nor_lb_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e4846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.006957766340111328\n"
     ]
    }
   ],
   "source": [
    "n_samples, dim1, dim2 = lb_new_data.shape\n",
    "lb_new_data_reshaped = lb_new_data.reshape(n_samples, dim1 * dim2)\n",
    "\n",
    "# Create and fit the KMeans model\n",
    "SA_Kmeans = KMeans(n_clusters=4, init='random', n_init=10, random_state=0)\n",
    "SA_test = SA_Kmeans.fit_predict(Nor_lb_padded)\n",
    "\n",
    "# Predict the clusters for the input data\n",
    "clusters = SA_Kmeans.predict(lb_new_data_reshaped)\n",
    "\n",
    "# Retrieve the cluster centroids\n",
    "centroids = SA_Kmeans.cluster_centers_\n",
    "\n",
    "# Calculate the MSE\n",
    "mse = mean_squared_error(lb_new_data_reshaped, centroids[clusters])\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430c2cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9433962264150944\n",
      "SA Accuracy: 0.5849056603773585\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0  0]\n",
      " [ 0  7  0  0]\n",
      " [ 1  1 12  1]\n",
      " [ 0  0  0 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       0.88      1.00      0.93         7\n",
      "           2       1.00      0.80      0.89        15\n",
      "           3       0.96      1.00      0.98        25\n",
      "\n",
      "    accuracy                           0.94        53\n",
      "   macro avg       0.92      0.95      0.93        53\n",
      "weighted avg       0.95      0.94      0.94        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple function to align cluster labels with true labels\n",
    "def align_cluster_labels_with_ground_truth(cluster_labels, true_labels):\n",
    "    aligned_labels = np.zeros_like(cluster_labels)\n",
    "    for i in np.unique(cluster_labels):\n",
    "        mask = (cluster_labels == i)\n",
    "        if np.sum(mask) == 0:  # This should handle the error, but ideally, this condition shouldn't occur\n",
    "            continue\n",
    "        aligned_labels[mask] = np.bincount(true_labels[mask]).argmax()\n",
    "    return aligned_labels\n",
    "\n",
    "aligned_cluster_labels = align_cluster_labels_with_ground_truth(lb_test_cluster_labels, labels_Int)\n",
    "\n",
    "SA_aligned_cluster_labels = align_cluster_labels_with_ground_truth(SA_test, labels_Int)\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(labels_Int, aligned_cluster_labels)\n",
    "#nmi = normalized_mutual_info_score(labels_Int, aligned_cluster_labels)\n",
    "SA_accuracy = accuracy_score(labels_Int, SA_aligned_cluster_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"SA Accuracy: {SA_accuracy}\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(labels_Int, aligned_cluster_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "report = classification_report(labels_Int, aligned_cluster_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffce47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Davies-Bouldin score: 0.6013748031973987\n"
     ]
    }
   ],
   "source": [
    "# Print the clustering evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "print('Test set Davies-Bouldin score:', davies_bouldin_score(exp_lb_encoded_test, aligned_cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96af9eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA Kmeans Test set Davies-Bouldin score: 1.6360604776982381\n"
     ]
    }
   ],
   "source": [
    "print('SA Kmeans Test set Davies-Bouldin score:', davies_bouldin_score(lb_new_data_reshaped, SA_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a82cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ca8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796682d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
